<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Elements Reference - Massively by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">SIPRADV</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">INICIO</a></li>
							<li><a href="generic.html">Bitácora</a></li>
							<li><a href="elements.html">Informe</a></li>
							<li><a href="Index2.html">SIPRADV 2.0</a></li>
							<li><a href="generic2.html">Bitácora 2</a></li>
							<li class="active"><a href="elements.html">Informe 2</a></li>
						</ul>
						<ul class="icons">
							<li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<h1>Informe<br />
									</h1>
								</header>

								<!-- Text stuff -->
									<h2>Resumen</h2>
									<p style="line-height: 1.0; margin-bottom: 5px">Aproximadamente 338 millones de personas a nivel mundial presentan problemas de visión severos.</p>
<p style="line-height: 1.0; margin-bottom: 5px;">Estas personas enfrentan desafíos significativos al salir de sus casas, ya que el mundo está diseñado para quienes pueden ver. </p>
<p style="line-height: 1.0; margin-bottom: 5px;">Para ayudar a mejorar la calidad de vida de estas personas, hemos desarrollado un Sistema Integrado de Percepción y Respuesta Auditiva para Discapacitados Visuales (SIPRADV). </p>
<p style="line-height: 1.0; margin-bottom: 5px;">SIPRADV detecta y alerta al usuario de la presencia de objetos en la periferia mejorando así la percepción del entorno.</p>
<p style="line-height: 1.0; margin-bottom: 5px;">Lo que permite a las personas con discapacidad visual desplazarse de manera más segura y autónoma.</p>
<p style="line-height: 1.0; margin-bottom: 5px;">El SIPRADV no fue suficiente para nosotros por lo que decidimos mejorarlo e integrarle mejor tecnología a través de mejoras en el controlador,</p>
<p style="line-height: 1.0; margin-bottom: 5px;">una nueva forma de detectar objetos de manera que no solo podamos saber que el objeto existe si no que es ese objeto. </p>
<p style="line-height: 1.0; margin-bottom: 5px;">Todo esto a través de una cámara. Dándole vida a la versión 2.0.</p>

									<hr />
									<h2>Pregunta de investigación </h2>
									<h3>¿Cómo podemos mejorar el SIPRADV para convertirlo en un sistema con mayor utilidad y eficiencia utilizando el reconocimiento de imágenes?
</h3>
									
									<hr />
									<header>
										<h2>Marco Teórico</h2>
										

<p>1. Percepción Espacial en Personas No videntes
La percepción espacial es fundamental para la movilidad y la orientación en el entorno. En personas no videntes, esta percepción se basa en gran medida en otros sentidos, como el oído, para formar un mapa mental del entorno. La investigación ha demostrado que las personas no videntes desarrollan una aguda capacidad para detectar y localizar sonidos, lo que les ayuda a navegar y evitar obstáculos. (Tactile, 2018)
</p>

<p>2. Tecnologías Asistidas para la Movilidad
Las tecnologías asistidas han avanzado significativamente en las últimas décadas, ofreciendo nuevas formas de apoyo para las personas con discapacidades visuales. Sistemas como los bastones electrónicos y dispositivos de navegación portátiles ayudan a mejorar la independencia y seguridad. Sin embargo, la integración de múltiples tecnologías, como sensores ultrasónicos y retroalimentación auditiva, puede ofrecer una solución más eficaz y personalizada. (Smith & Johnson, 2020). 
</p>

<p>3. Sensores Ultrasónicos en Tecnología de Asistencia
Los sensores ultrasónicos, como el GY-US42V2, son dispositivos que utilizan ondas sonoras de alta frecuencia para medir distancias y detectar la presencia de objetos. Estos sensores ofrecen ventajas sobre las tecnologías más antiguas al proporcionar una mayor precisión y capacidad de medición en diferentes condiciones. Los modelos avanzados, como el GY-US42V2, ofrecen opciones de comunicación como UART y PWM, lo que mejora la integración con sistemas electrónicos complejos. (Lee et al., 2022) 
</p>

<p>4. Audio Espacial y Feedback Auditivo
El audio espacial es una técnica que utiliza múltiples fuentes de sonido para crear una experiencia auditiva tridimensional. En el contexto de la asistencia a personas ciegas, el audio espacial puede proporcionar información sobre la dirección y la proximidad de los objetos, permitiendo una navegación más intuitiva. El uso de sistemas de audio direccional puede mejorar la capacidad de una persona para localizar obstáculos y adaptarse a su entorno con mayor facilidad. (Miller, 2019) 
</p>

<p>5. Integración de Sensores y Sistemas de Audio en Dispositivos de Asistencia
La integración de sensores ultrasónicos con sistemas de audio espacial puede ofrecer un enfoque más eficaz para la navegación asistida. Al combinar la detección precisa de obstáculos con la retroalimentación auditiva direccional, se puede proporcionar a las personas ciegas una percepción más clara de su entorno. Este enfoque combina la tecnología de sensores avanzados con técnicas de retroalimentación auditiva para crear una solución de asistencia más completa y funcional. (Wang & Chen, 2021).
</p>

<p>6. Cámaras Digitales
Las cámaras digitales son dispositivos electrónicos que capturan imágenes en formato digital mediante sensores electrónicos, generalmente sensores CCD (Charge Coupled Device) o CMOS (Complementary Metal Oxide Semiconductor). Estas imágenes se utilizan como datos de entrada para sistemas de procesamiento y análisis visual.
Funcionamiento básico: La cámara convierte la luz capturada a través del lente en señales eléctricas, que luego se digitalizan para formar una imagen.
Tipos de cámaras usadas: Webcams, cámaras IP, cámaras industriales, cámaras móviles, entre otras, que varían en resolución, velocidad de captura y sensibilidad.
</p>

<p>7. Reconocimiento de Objetos
El reconocimiento de objetos es una subárea del campo de la visión por computadora e inteligencia artificial que busca identificar y clasificar objetos presentes en imágenes o videos.
Definición: Consiste en localizar y etiquetar objetos de interés dentro de una imagen o secuencia de imágenes.
Aplicaciones: Seguridad, robótica, vehículos autónomos, sistemas de monitoreo, realidad aumentada, diagnóstico médico, entre otros. 
</p>

<p>8. Procesamiento de la Imagen
El proceso típico para el reconocimiento de objetos incluye:
Captura de imagen: Mediante cámara digital.
Preprocesamiento: Mejoras en la calidad de imagen, reducción de ruido, normalización.
Detección: Localización aproximada del objeto dentro de la imagen.
Clasificación: Asignación de una etiqueta o categoría al objeto detectado.
Post procesamiento: Refinamiento, eliminación de falsos positivos y presentación de resultados.<p>

<p>9. Contexto nacional
En Panamá, la Universidad Tecnológica de Panamá (UTP) ha sido pionera en la investigación y desarrollo de tecnologías de asistencia para personas con discapacidad visual. Un estudio relevante llevado a cabo en la UTP exploró el uso de dispositivos de detección de obstáculos basados en inteligencia artificial. Los resultados de este estudio fueron prometedores, demostrando la eficacia de la tecnología en la identificación y alerta de obstáculos en tiempo real.
Sin embargo, el estudio también identificó un desafío importante: el tamaño del dispositivo. Los prototipos desarrollados, aunque funcionales, eran voluminosos y afectan la comodidad de los usuarios durante su uso prolongado. Esta limitación resalta la necesidad de continuar la investigación y desarrollo en esta área, con un enfoque particular en la miniaturización y ergonomía de los dispositivos, para garantizar una experiencia de usuario óptima. (Rodríguez, 2023)
</p>


										
									
										<h2>Hipótesis</h2>
										<p>El prototipo será capaz de reducir el peligro de las personas visualmente discapacitadas y les dará una mejor percepción espacial de su entorno. El sistema demostrará mayor eficiencia y mejores funciones.</p>

										<h2 style="font-size: 1.5em;"><strong>Metodología</strong></h2>

<strong>Materiales Necesarios:</strong>
<ul>
  <li>1 - Raspberry Pi 5</li>
  <li>1 - Cooling case</li>
  <li>1 - 128 GB Memory</li>
  <li>1 - Raspberry Pi 5 Charger</li>
  <li>1 - Speaker</li>
  <li>1 - Headphones</li>
  <li>1 - 30,000 mAh power bank</li>
  <li>1 - Google Coral USB</li>
  <li>1 - Webcam C270 Logitech</li>
</ul>

<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Diagramas</title>
</head>


  <h2>Diagrama físico del sistema</h2>
	</header>
<div class="images/diagrama.jpg"><img src="images/diagrama.jpg" alt="" /></div>
	<div style="text-align: left;">
								
						


  <h2>Diagrama conceptual</h2>
<p><img src="https://usuario.github.io/miweb/images/diagrama2.jpg" <p>



<strong>Resultados</strong>
<p>Los resultados que obtuvimos gracias al sistema basado en Raspberry Pi 5, reconocimiento de objetos y un modelo de lenguaje demuestra cómo podemos unir varias áreas de la tecnología juntas como la visión por computadora y procesamiento de lenguaje natural en un mismo dispositivo de manera local. Este logro ejecutar las instrucciones y objetivo proyectado. El Raspberry Pi 5 ejecutaba la detección de objetos en tiempo real de manera eficaz, precisa y fluida. Posteriormente, el LLM interpretó la información recolectada por la cámara y le dio descripciones comprensibles y contextuales que ayudan al usuario. Este abre posibilidades para aplicaciones de asistencia, navegación autónoma y educación sin necesidad de conexión a internet. No es un sistema perfecto, sin embargo logramos entender que es posible crear sistemas como este para facilitar el día a día a las personas que más lo necesitan.
<p>										
		
<p>Con base en los resultados obtenidos, se proponen las siguientes recomendaciones para mejorar el prototipo y su funcionalidad:</p>

<strong>Implementar un modelo adaptado al entorno local</strong>
<p>Es importante que el sistema utilice un modelo entrenado y ajustado específicamente para las condiciones de Panamá. Esto incluye no sólo el contexto urbano y ambiental, sino también la adaptación a las necesidades de los usuarios que se desplazan y transitan diariamente por las calles.</p>

<strong>Reducir el tamaño del dispositivo</strong>
<p>Se recomienda trabajar en la disminución de las dimensiones y el peso del sistema, de manera que sea más cómodo y ergonómico para el usuario. Esto contribuirá a que el dispositivo sea menos invasivo y más práctico para su uso diario.
</p>

<strong>Optimizar el sistema de audio</strong>
<p>Es fundamental que la retroalimentación auditiva sea más clara, intuitiva y fácil de interpretar. Un sistema de audio mejorado permitirá que los usuarios comprendan de forma inmediata la información que reciben, facilitando la toma de decisiones rápidas y seguras en su desplazamiento.
</p>


									<h2>Referencias bibliográficas</h2>
								
    <li>Getting Started with YOLO Object and Animal Recognition on the Raspberry Pi - Tutorial Australia. (2024, September 27). Core Electronics. https://core-electronics.com.au/guides/raspberry-pi/getting-started-with-yolo-object-and-animal-recognition-on-the-raspberry-pi/</li>
    <li>DIY Engineers. (2024, October 25). Boost Your AI Game on Raspberry Pi with Coral USB TPU – Full Setup Guide. YouTube. https://www.youtube.com/watch?v=TRxR74qA-5E</li>
    <li>Core Electronics. (2024, September 26). YOLO Object and Animal Recognition on the Raspberry Pi 5 | Beginner Python Guide. YouTube. https://www.youtube.com/watch?v=XKIm_R_rIeQ</li>
    <li>Ultralytics. (n.d.). COCO. Docs.ultralytics.com. Retrieved April 22, 2024, from https://docs.ultralytics.com/datasets/detect/coco/#sample-images-and-annotations</li>
    <li>Taghipour Zarfsaz, B. (2021). A study on dual-memory architectures in incremental learning (Figure 3) [Figure]. ResearchGate. Retrieved from https://www.researchgate.net/figure/Object-detection-in-street-Source-5_fig3_350153670</li>
		<li>Ultralytics. (n.d.). How to run YOLOv8: Easy setup guide. YOLOv8. Retrieved August 21, 2025, from https://docs.ultralytics.com</li>
										


									<hr />

								<!-- Lists -->
									

										
											

									

									

								<!-- Table -->
									
												
												
									

								<!-- Form -->
									
												
											

								<!-- Image -->
									

								<!-- Box -->
									

								<!-- Preformatted Code -->


							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>Colegio Episcopal de Panamá<br />
								Calle Carlos Arias, Nuevo Reparto El Carmen</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">(+507) 6417-7938</a></p>
								<p><a href="#">(+507) 6934-4590</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="#">haidarrada21@gmail.com</a></p>
								<p><a href="#">Liangmach@hotmail.com</a></p>
							</section>
							
					

				<!-- Copyright -->
					

			

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>





